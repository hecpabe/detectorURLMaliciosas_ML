{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción**\n",
        "En este notebook vamos a implementar un modelo que detecte URLs maliciosas, para ello obtendremos un dataset de la siguiente dirección: https://raw.githubusercontent.com/saurabhsingh0/malacious-url-detector/master/model/datasets/malicious-url-detector_dataset.csv\n",
        "\n",
        "\n",
        "---\n",
        "Para abordar el problema he encontrado que se suelen utilizar 3 modelos diferentes:\n",
        "\n",
        "\n",
        "*   *Naive Bayes Classifier*\n",
        "*   *N-Gram with Naive Bayes Classifier*\n",
        "*   *N-Gram with SVM Classifier*\n",
        "\n",
        "He seleccionado el tercero por dos motivos:\n",
        "\n",
        "\n",
        "1.   Naive Bayes ya lo hemos utilizado más veces en clase y además ya le he dado un sitio en el trabajo, a SVM no, así diversifico más y aprendo otros puntos de vista.\n",
        "2.   He podido ver que el modelo de SVM funciona mejor para los datasets grandes como el nuestro.\n",
        "\n"
      ],
      "metadata": {
        "id": "peG7e43ohUox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importamos las dependencias**"
      ],
      "metadata": {
        "id": "vBe1ZP0SkCej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZgNv-FerhOqI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from joblib import dump, load\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creamos los N-Grams**\n",
        "\n",
        "Creamos las posibles combinaciones de N-Grams usando el alfabeto (inglés) en minúscula junto a los dígitos en un diccionario de Python"
      ],
      "metadata": {
        "id": "J-HNNSE6k_kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphanum = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z','0','1','2','3','4','5','6','7','8','9']\n",
        "permutations = itertools.product(alphanum, repeat=3)\n",
        "featuresDict = {}\n",
        "counter = 0\n",
        "for perm in permutations:\n",
        "    #print(perm)\n",
        "    f=''\n",
        "    for char in perm:\n",
        "        f = f+char;\n",
        "    featuresDict[(''.join(perm))] = counter\n",
        "    counter = counter + 1"
      ],
      "metadata": {
        "id": "bMutg8MGlVnD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generamos los N-Gram**\n",
        "\n",
        "Creamos una función que nos permita transformar un string en una lista de N-Gram"
      ],
      "metadata": {
        "id": "3XKIwA1Dl3tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngram(sentence):\n",
        "    s = sentence.lower()\n",
        "    s = ''.join(e for e in s if e.isalnum())\n",
        "    processedList = []\n",
        "    for tup in list(ngrams(s,3)):\n",
        "        processedList.append((''.join(tup)))\n",
        "    return processedList"
      ],
      "metadata": {
        "id": "_E-T92Kjmino"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocesado de los datos**\n",
        "\n",
        "Creamos una función para preprocesar el dataset en formatos que nuestro modelo pueda aceptar y limpiar las URL de caracteres no deseados."
      ],
      "metadata": {
        "id": "a0adRcZrmrn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentences(dataframe, X, y):\n",
        "    #print(dataframe)\n",
        "    for index,row in df.iterrows():\n",
        "        url = row['url'].strip().replace(\"https://\",\"\")\n",
        "        url = row['url'].strip().replace(\"http://\",\"\")\n",
        "        url = url.replace(\"http://\",\"\")\n",
        "        url = re.sub(r'\\.[A-Za-z0-9]+/*','',url)\n",
        "        for gram in generate_ngram(url):\n",
        "            try:\n",
        "                X[index][featuresDict[gram]] = X[index][featuresDict[gram]] + 1\n",
        "            except:\n",
        "                print(gram,\"no existe\")\n",
        "        y[index] = int(row['label'])\n",
        "    return (X,y)"
      ],
      "metadata": {
        "id": "n7fHKivnnm5z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gestión del dataset**\n",
        "\n",
        "Cargamos el dataset y lo separamos en los conjuntos de train y test"
      ],
      "metadata": {
        "id": "xlVXlS-JoOSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_dataframe = pd.read_csv(\"malicious-url-detector_dataset.csv\")\n",
        "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)"
      ],
      "metadata": {
        "id": "f_30i2FGoatp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gestión del modelo**\n",
        "\n",
        "Vamos a utilizar SGDClassifier, el cual utiliza por defecto un SVM Lineal, que era el que queríamos utilizar, para el aprendizaje utilizaremos SGD (Stochastic Gradient Descent).\n",
        "\n",
        "Utilizaremos un entrenamiento parcial."
      ],
      "metadata": {
        "id": "MFjGDbjSo3DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_rows = 5000\n",
        "no_of_batches = int(train_df.shape[0]/no_of_rows) +1\n",
        "classifier = SGDClassifier()\n",
        "#print(no_of_batches)\n",
        "for i in range(0, no_of_batches):\n",
        "    start = no_of_rows*i\n",
        "    if start + no_of_rows > train_df.shape[0] :\n",
        "        df = train_df.iloc[start:,:]\n",
        "    else :\n",
        "        df = train_df.iloc[start:start+no_of_rows, :]\n",
        "    df = df.reset_index()\n",
        "    (X,y) = preprocess_sentences(df, \\\n",
        "                                 np.zeros([df.shape[0], 46656],dtype=\"int\"), \\\n",
        "                                 np.zeros(df.shape[0],dtype=\"int\"))\n",
        "    classifier.partial_fit(X, y, classes=np.unique(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONwQ-F5rpwos",
        "outputId": "3ec83e25-26f8-47fd-8dcf-ae5058d144c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wnو no existe\n",
            "nو² no existe\n",
            "و²³ no existe\n",
            "²³2 no existe\n",
            "³2b no existe\n",
            "e4ä no existe\n",
            "4ää no existe\n",
            "ääš no existe\n",
            "äšç no existe\n",
            "šçè no existe\n",
            "çèç no existe\n",
            "èçˆ no existe\n",
            "çˆï no existe\n",
            "ˆï¼ no existe\n",
            "ï¼ˆ no existe\n",
            "¼ˆå no existe\n",
            "ˆåå no existe\n",
            "ååº no existe\n",
            "åºå no existe\n",
            "ºåˆ no existe\n",
            "åˆå no existe\n",
            "ˆåï no existe\n",
            "åï¼ no existe\n",
            "ï¼3 no existe\n",
            "¼32 no existe\n",
            "azä no existe\n",
            "zä½ no existe\n",
            "ä½³ no existe\n",
            "½³è no existe\n",
            "³èƒ no existe\n",
            "èƒ½ no existe\n",
            "ƒ½ç no existe\n",
            "½çå no existe\n",
            "çåš no existe\n",
            "åšæ no existe\n",
            "šæå no existe\n",
            "æåè no existe\n",
            "åè½ no existe\n",
            "è½ä no existe\n",
            "½äc no existe\n",
            "äca no existe\n",
            "16ه no existe\n",
            "6هé no existe\n",
            "هéœ no existe\n",
            "éœ² no existe\n",
            "œ²ه no existe\n",
            "²هه no existe\n",
            "ههه no existe\n",
            "ههچ no existe\n",
            "هچˆ no existe\n",
            "چˆه no existe\n",
            "ˆهœ no existe\n",
            "هœ6 no existe\n",
            "œ60 no existe\n",
            "18æ no existe\n",
            "8æ³ no existe\n",
            "æ³å no existe\n",
            "³åœ no existe\n",
            "åœæ no existe\n",
            "œæœ no existe\n",
            "æœº no existe\n",
            "œº3 no existe\n",
            "º32 no existe\n",
            "azä no existe\n",
            "zä½ no existe\n",
            "ä½³ no existe\n",
            "½³è no existe\n",
            "³èƒ no existe\n",
            "èƒ½ no existe\n",
            "ƒ½ç no existe\n",
            "½çå no existe\n",
            "çåš no existe\n",
            "åšæ no existe\n",
            "šæå no existe\n",
            "æåè no existe\n",
            "åè½ no existe\n",
            "è½ä no existe\n",
            "½äc no existe\n",
            "äca no existe\n",
            "aiه no existe\n",
            "iه½ no existe\n",
            "ه½è no existe\n",
            "½è¹ no existe\n",
            "è¹ه no existe\n",
            "¹هن no existe\n",
            "هن½ no existe\n",
            "ن½ه no existe\n",
            "½ه½ no existe\n",
            "ه½è no existe\n",
            "½è¹ no existe\n",
            "è¹ه no existe\n",
            "¹هڈ no existe\n",
            "هڈو no existe\n",
            "ڈوه no existe\n",
            "وهç no existe\n",
            "هç² no existe\n",
            "ç²ن no existe\n",
            "²نه no existe\n",
            "نهé no existe\n",
            "هéن no existe\n",
            "éن½ no existe\n",
            "e4ä no existe\n",
            "4ää no existe\n",
            "ääš no existe\n",
            "äšç no existe\n",
            "šçè no existe\n",
            "çèç no existe\n",
            "èçˆ no existe\n",
            "çˆï no existe\n",
            "ˆï¼ no existe\n",
            "ï¼ˆ no existe\n",
            "¼ˆå no existe\n",
            "ˆåå no existe\n",
            "ååº no existe\n",
            "åºå no existe\n",
            "ºåˆ no existe\n",
            "åˆå no existe\n",
            "ˆåï no existe\n",
            "åï¼ no existe\n",
            "ï¼3 no existe\n",
            "¼32 no existe\n",
            "aiه no existe\n",
            "iه½ no existe\n",
            "ه½è no existe\n",
            "½è¹ no existe\n",
            "è¹ه no existe\n",
            "¹هن no existe\n",
            "هن½ no existe\n",
            "ن½ه no existe\n",
            "½ه½ no existe\n",
            "ه½è no existe\n",
            "½è¹ no existe\n",
            "è¹ه no existe\n",
            "¹هڈ no existe\n",
            "هڈو no existe\n",
            "ڈوه no existe\n",
            "وهç no existe\n",
            "هç² no existe\n",
            "ç²ن no existe\n",
            "²نه no existe\n",
            "نهé no existe\n",
            "هéن no existe\n",
            "éن½ no existe\n",
            "wnا no existe\n",
            "nاه no existe\n",
            "اهء no existe\n",
            "هءم no existe\n",
            "wnو no existe\n",
            "nو² no existe\n",
            "و²³ no existe\n",
            "²³2 no existe\n",
            "³2b no existe\n",
            "16ه no existe\n",
            "6هé no existe\n",
            "هéœ no existe\n",
            "éœ² no existe\n",
            "œ²ه no existe\n",
            "²هه no existe\n",
            "ههه no existe\n",
            "ههچ no existe\n",
            "هچˆ no existe\n",
            "چˆه no existe\n",
            "ˆهœ no existe\n",
            "هœ6 no existe\n",
            "œ60 no existe\n",
            "18æ no existe\n",
            "8æ³ no existe\n",
            "æ³å no existe\n",
            "³åœ no existe\n",
            "åœæ no existe\n",
            "œæœ no existe\n",
            "æœº no existe\n",
            "œº3 no existe\n",
            "º32 no existe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Realizamos el test del modelo**"
      ],
      "metadata": {
        "id": "EMLOgGU6sjKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_rows = 5000\n",
        "no_of_batches = int(test_df.shape[0]/no_of_rows) +1\n",
        "#print(no_of_batches)\n",
        "correct = 0;\n",
        "incorrect = 0\n",
        "for i in range(0, no_of_batches):\n",
        "    start = no_of_rows*i\n",
        "    if start + no_of_rows > train_df.shape[0] :\n",
        "        df = train_df.iloc[start:,:]\n",
        "    else :\n",
        "        df = test_df.iloc[start:start+no_of_rows, :]\n",
        "    df = df.reset_index()\n",
        "    (X_test,y_test) = preprocess_sentences(df, \\\n",
        "                                 np.zeros([df.shape[0], 46656],dtype=\"int\"), \\\n",
        "                                 np.zeros(df.shape[0],dtype=\"int\"))\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    for index,row in df.iterrows():\n",
        "        if row['label'] == y_pred[index]:\n",
        "            correct = correct+1\n",
        "        else:\n",
        "            incorrect = incorrect + 1   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Qw1a2kstj4",
        "outputId": "71647c2a-56f9-4b0f-847f-1b8166bc64cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reä no existe\n",
            "eää no existe\n",
            "ääš no existe\n",
            "äšç no existe\n",
            "šçˆ no existe\n",
            "çˆå no existe\n",
            "ˆåœ no existe\n",
            "åœç no existe\n",
            "œç¾ no existe\n",
            "ç¾ž no existe\n",
            "¾žç no existe\n",
            "žçè no existe\n",
            "çèç no existe\n",
            "èçˆ no existe\n",
            "çˆ3 no existe\n",
            "ˆ32 no existe\n",
            "reä no existe\n",
            "eää no existe\n",
            "ääš no existe\n",
            "äšç no existe\n",
            "šçˆ no existe\n",
            "çˆå no existe\n",
            "ˆåœ no existe\n",
            "åœç no existe\n",
            "œç¾ no existe\n",
            "ç¾ž no existe\n",
            "¾žç no existe\n",
            "žçè no existe\n",
            "çèç no existe\n",
            "èçˆ no existe\n",
            "çˆ3 no existe\n",
            "ˆ32 no existe\n",
            "wnا no existe\n",
            "nاه no existe\n",
            "اهء no existe\n",
            "هءم no existe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comprobamos los resultados**"
      ],
      "metadata": {
        "id": "0mOXJutVs9ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicciones correctas: \", correct)\n",
        "print(\"Predicciones incorrectas: \", incorrect)\n",
        "accuracy = (correct/test_df.shape[0])*100\n",
        "print(\"Precisión: \"'{0:.4g}'.format(accuracy), '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuLamTmUtLI-",
        "outputId": "1843d0b8-d062-466a-b0d2-52bbe64fb2d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicciones correctas:  206679\n",
            "Predicciones incorrectas:  3036\n",
            "Precisión: 98.55 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Probamos con URLs a mano**"
      ],
      "metadata": {
        "id": "qbmlG2OPyn12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una funcion para preprocesar una única URL"
      ],
      "metadata": {
        "id": "OLybUbwsyrFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentences_url(url):\n",
        "    X= np.zeros([1, 46656],dtype=\"int\")\n",
        "    url = url.strip().replace(\"https://\",\"\")\n",
        "    url = url.replace(\"http://\",\"\")\n",
        "    url = re.sub(r'\\.[A-Za-z0-9]+/*','',url)\n",
        "    for gram in generate_ngram(url):\n",
        "        try:\n",
        "            X[0][featuresDict[gram]] = X[0][featuresDict[gram]] + 1\n",
        "            #print('preprocess_sentences')\n",
        "            #print(X[index][featuresDict[gram]])\n",
        "        except:\n",
        "            print(gram,\"no existe\")\n",
        "    return X        "
      ],
      "metadata": {
        "id": "KJ1rIDDayvVU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probamos a introducir una URL maliciosa"
      ],
      "metadata": {
        "id": "HezWJ0tIy5DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"www.itidea.it/centroesteticosothys/img/_notes/gum.exe\"\n",
        "test = preprocess_sentences_url(url)\n",
        "pred = classifier.predict(test)\n",
        "if pred == 1 :\n",
        "    print(url, \" es una URL maliciosa\")\n",
        "else:\n",
        "    print(url, \" es una URL segura\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06cULj0Cy8ZH",
        "outputId": "064df888-416d-4468-c155-a063d247942b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "www.itidea.it/centroesteticosothys/img/_notes/gum.exe  es una URL maliciosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probamos a introducir una URL segura"
      ],
      "metadata": {
        "id": "5mcE0pq0zQL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://u-tad.com/\"\n",
        "test = preprocess_sentences_url(url)\n",
        "pred = classifier.predict(test)\n",
        "if pred == 1 :\n",
        "    print(url, \" es una URL maliciosa\")\n",
        "else:\n",
        "    print(url, \" es una URL segura\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci1GNTTgzSSC",
        "outputId": "e930c223-5819-485b-9a72-9d8988aedcde"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://u-tad.com/  es una URL segura\n"
          ]
        }
      ]
    }
  ]
}